{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dccb61",
   "metadata": {},
   "source": [
    "Data sourced from Kaggle competition [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d54f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f6db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992c4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics/evaluation\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f9a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the train and test sets\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test =  pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b056ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â combining the train and test sets for the purpose of EDA and Data Cleaning/Feature Engineering\n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff5bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataframe Shape: (7613, 5)\n",
      "Test Dataframe Shape: (3263, 4)\n",
      "Combined Dataframe Shape: (10876, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataframe Shape: {}\".format(str(train.shape)))\n",
    "print(\"Test Dataframe Shape: {}\".format(str(test.shape)))\n",
    "print(\"Combined Dataframe Shape: {}\".format(str(df.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0e5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tweets\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf5faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10876 entries, 0 to 10875\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        10876 non-null  int64  \n",
      " 1   keyword   10789 non-null  object \n",
      " 2   location  7238 non-null   object \n",
      " 3   text      10876 non-null  object \n",
      " 4   target    7613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 425.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4aaee5",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7524c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       87\n",
       "location    3638\n",
       "text           0\n",
       "target      3263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values in the training set\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76503b",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790ea358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8022c5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    42\n",
       "0.0    19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to understand whether the null values in keyword have any relevance - they don't\n",
    "\n",
    "df[df.keyword.isnull()].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcc08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new category for the null keyword and location values\n",
    "\n",
    "df.fillna({'keyword': 'unknown', 'location': 'unknown'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080f63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the keyword column\n",
    "\n",
    "df.replace({'keyword': '%20'}, {'keyword': '_'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cda27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting keyword column to binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b56ce0",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae61b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown            3638\n",
       "USA                 141\n",
       "New York            109\n",
       "United States        65\n",
       "London               58\n",
       "Canada               42\n",
       "Nigeria              40\n",
       "Worldwide            35\n",
       "India                35\n",
       "Los Angeles, CA      34\n",
       "UK                   33\n",
       "Kenya                32\n",
       "Washington, DC       31\n",
       "Mumbai               28\n",
       "United Kingdom       26\n",
       "California           25\n",
       "Australia            25\n",
       "Los Angeles          24\n",
       "Chicago, IL          23\n",
       "San Francisco        23\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given how messy and the location column is, it's unlikely that we'll be able to clean it for modelling purposes\n",
    "\n",
    "df.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e0291",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c30c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all tweets are in English\n",
    "\n",
    "# lang_series = df.text.apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbb31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lang_series as a joblib file\n",
    "\n",
    "# joblib.dump(lang_series, 'jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88fd48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading lang_series jlib file\n",
    "\n",
    "lang_series = joblib.load('jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4c25239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = lang_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09e030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>10474</td>\n",
       "      <td>wild_fires</td>\n",
       "      <td>unknown</td>\n",
       "      <td>WILD FIRES! http://t.co/EgrMdkXpOi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>3698</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Media stocks are getting destroyed (DIS FOXA C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1267</td>\n",
       "      <td>blood</td>\n",
       "      <td>PunPunlÃÂ¢ndia</td>\n",
       "      <td>@Lobo_paranoico Mad Men</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>1048</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>Nice places</td>\n",
       "      <td>@King_Naruto_ As long as I see Madara bleeding...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>10272</td>\n",
       "      <td>war_zone</td>\n",
       "      <td>We're All Mad Here</td>\n",
       "      <td>Packing for CT aka my room looks like a war zone</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     keyword            location  \\\n",
       "10768  10474  wild_fires             unknown   \n",
       "8731    3698   destroyed             unknown   \n",
       "874     1267       blood       PunPunlÃÂ¢ndia   \n",
       "725     1048    bleeding        Nice places    \n",
       "7168   10272    war_zone  We're All Mad Here   \n",
       "\n",
       "                                                    text  target language  \n",
       "10768                 WILD FIRES! http://t.co/EgrMdkXpOi     NaN       de  \n",
       "8731   Media stocks are getting destroyed (DIS FOXA C...     NaN       sv  \n",
       "874                              @Lobo_paranoico Mad Men     0.0       es  \n",
       "725    @King_Naruto_ As long as I see Madara bleeding...     0.0       tl  \n",
       "7168    Packing for CT aka my room looks like a war zone     0.0       pl  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.language != 'en'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88469b7",
   "metadata": {},
   "source": [
    "It seems that the language detector function isn't doing a very good job of picking up some of the tweets' language. Regardless, it seems that all of the tweets are in English so we don't have to worry about dealing with other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a1d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping language column from dataset\n",
    "\n",
    "df.drop('language', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f9b66",
   "metadata": {},
   "source": [
    "### Using the tweet-preprocessor package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa57d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14114371",
   "metadata": {},
   "source": [
    "### Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "823df9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the tweet characteristics below from the tweets\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b109fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wholesale Markets ablaze'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.clean(df.text[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c781418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df.text.apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93410d0",
   "metadata": {},
   "source": [
    "### Creating meta-data for tweet characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f784e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenized = df.text.apply(lambda x:p.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab7e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for-loop to add columns for the tweet meta-data features\n",
    "\n",
    "for feature in ['url', 'smiley', 'mention']:\n",
    "    feature_counter = []\n",
    "    for tweet in tweet_tokenized:\n",
    "        counter = 0\n",
    "        for word in tweet.split():\n",
    "            if word == \"$\"+feature.upper()+\"$\":\n",
    "                counter += 1\n",
    "        feature_counter.append(counter)\n",
    "    df[\"tweet_\"+feature] = feature_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c922a",
   "metadata": {},
   "source": [
    "### Meta-data for hash-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f97ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_counter(x):\n",
    "    counter = 0\n",
    "    for word in x.split():\n",
    "        if word[0] == '#' and len(word) > 1:\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cec9b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_hashtag'] = df.text_clean.apply(hash_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed2c20",
   "metadata": {},
   "source": [
    "### Text meta-data: length of tweet, number of words and average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b8d806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5965494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_characters'] = df.text_clean.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d60b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(tweet):   \n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()      \n",
    "    return len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93c045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_words'] = df.text_clean.apply(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96421055",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''.join([x for x in 'Our Deeds are the Reason of this #earthquake ' if x not in string.punctuation]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8eeaad9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, words))/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0304654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_word_length(tweet):\n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()\n",
    "    return sum(map(len, word_lst))/len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be3650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_av_word_length'] = df.text_clean.apply(ave_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196de66b",
   "metadata": {},
   "source": [
    "### Remove punctuation completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8586a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation from tweets\n",
    "\n",
    "for punct in string.punctuation:\n",
    "    df['text_clean'] = df.text_clean.str.replace(punct,'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d754958",
   "metadata": {},
   "source": [
    "###Â Removing digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9efdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_num'] = df.text_clean.replace('\\d+','',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807d274",
   "metadata": {},
   "source": [
    "### Expanding contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb7afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_contr'] = df.no_num.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd3b3b",
   "metadata": {},
   "source": [
    "### Tokenizing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c1665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text_no_contr.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0479d0a",
   "metadata": {},
   "source": [
    "### Change to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "832438f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = df.tokenized.apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7a8b7",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01936082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad08dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jradford/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62beefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1afdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stop'] = df.lower.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c42ba",
   "metadata": {},
   "source": [
    "### Beginning the lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1a9e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6947e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e804042e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['pos_tag'] = df.no_stop.apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f875a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11044ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('going', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "945cd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "df['wordnet_pos'] = df['pos_tag'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0548c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = df.wordnet_pos.apply(lambda x: ' '.join([wnl.lemmatize(word, tag) for word,tag in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "856c7b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_smiley</th>\n",
       "      <th>tweet_mention</th>\n",
       "      <th>tweet_hashtag</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_words</th>\n",
       "      <th>tweet_av_word_length</th>\n",
       "      <th>no_num</th>\n",
       "      <th>text_no_contr</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_stop</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[(deeds, NNS), (reason, NN), (earthquake, NN),...</td>\n",
       "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[(forest, JJS), (fire, NN), (near, IN), (la, J...</td>\n",
       "      <td>[(forest, a), (fire, n), (near, n), (la, a), (...</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4.954545</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[(residents, NNS), (asked, VBD), (shelter, JJ)...</td>\n",
       "      <td>[(residents, n), (asked, v), (shelter, a), (pl...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[(people, NNS), (receive, VBP), (wildfires, NN...</td>\n",
       "      <td>[(people, n), (receive, v), (wildfires, n), (e...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[(got, VBD), (sent, JJ), (photo, NN), (ruby, N...</td>\n",
       "      <td>[(got, v), (sent, a), (photo, n), (ruby, n), (...</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   1  unknown  unknown  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  unknown  unknown             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  unknown  unknown  All residents asked to 'shelter in place' are ...   \n",
       "3   6  unknown  unknown  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  unknown  unknown  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  tweet_url  \\\n",
       "0     1.0  Our Deeds are the Reason of this earthquake Ma...          0   \n",
       "1     1.0              Forest fire near La Ronge Sask Canada          0   \n",
       "2     1.0  All residents asked to shelter in place are be...          0   \n",
       "3     1.0  13000 people receive wildfires evacuation orde...          0   \n",
       "4     1.0  Just got sent this photo from Ruby Alaska as s...          0   \n",
       "\n",
       "   tweet_smiley  tweet_mention  tweet_hashtag  ...  tweet_words  \\\n",
       "0             0              0              1  ...           13   \n",
       "1             0              0              0  ...            7   \n",
       "2             0              0              0  ...           22   \n",
       "3             0              0              1  ...            8   \n",
       "4             0              0              2  ...           16   \n",
       "\n",
       "   tweet_av_word_length                                             no_num  \\\n",
       "0              4.307692  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1              4.428571              Forest fire near La Ronge Sask Canada   \n",
       "2              4.954545  All residents asked to shelter in place are be...   \n",
       "3              6.875000   people receive wildfires evacuation orders in...   \n",
       "4              4.375000  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                       text_no_contr  \\\n",
       "0  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  All residents asked to shelter in place are be...   \n",
       "3  people receive wildfires evacuation orders in ...   \n",
       "4  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, residents, asked, to, shelter, in, place...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                             no_stop  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(deeds, NNS), (reason, NN), (earthquake, NN),...   \n",
       "1  [(forest, JJS), (fire, NN), (near, IN), (la, J...   \n",
       "2  [(residents, NNS), (asked, VBD), (shelter, JJ)...   \n",
       "3  [(people, NNS), (receive, VBP), (wildfires, NN...   \n",
       "4  [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(deeds, n), (reason, n), (earthquake, n), (ma...   \n",
       "1  [(forest, a), (fire, n), (near, n), (la, a), (...   \n",
       "2  [(residents, n), (asked, v), (shelter, a), (pl...   \n",
       "3  [(people, n), (receive, v), (wildfires, n), (e...   \n",
       "4  [(got, v), (sent, a), (photo, n), (ruby, n), (...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  people receive wildfire evacuation order calif...  \n",
       "4  get sent photo ruby alaska smoke wildfires pou...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48063545",
   "metadata": {},
   "source": [
    "### Removing columns that are no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "addf6998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'text_clean',\n",
       "       'tweet_url', 'tweet_smiley', 'tweet_mention', 'tweet_hashtag',\n",
       "       'tweet_characters', 'tweet_words', 'tweet_av_word_length', 'no_num',\n",
       "       'text_no_contr', 'tokenized', 'lower', 'no_stop', 'pos_tag',\n",
       "       'wordnet_pos', 'lemmatized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "029ef561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['keyword', 'location', 'text', 'text_clean', 'no_num', 'text_no_contr',\n",
    "         'tokenized', 'lower', 'no_stop', 'pos_tag',\n",
    "         'wordnet_pos'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745c3ba",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a910ac",
   "metadata": {},
   "source": [
    "- seperate the below by each target variable\n",
    "    - number of characters in each tweet\n",
    "    - average word length in each sentence\n",
    "    - most commonly appearing ngrams of various lenghts\n",
    "    - textblob for sentiment analysis\n",
    "    - use speech tagging\n",
    "    - frequency of most common words\n",
    "    - number of words with a given number of appearances\n",
    "- word clouds for each target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19de1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_md = ['tweet_url', 'tweet_smiley', 'tweet_mention', 'tweet_hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8bf5a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_smiley</th>\n",
       "      <th>tweet_mention</th>\n",
       "      <th>tweet_hashtag</th>\n",
       "      <th>tweet_characters</th>\n",
       "      <th>tweet_words</th>\n",
       "      <th>tweet_av_word_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5276.446338</td>\n",
       "      <td>0.503224</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>0.399355</td>\n",
       "      <td>0.384846</td>\n",
       "      <td>78.244818</td>\n",
       "      <td>13.487333</td>\n",
       "      <td>4.767997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5661.608071</td>\n",
       "      <td>0.760012</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.254968</td>\n",
       "      <td>0.509324</td>\n",
       "      <td>86.497401</td>\n",
       "      <td>13.848059</td>\n",
       "      <td>5.207632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  tweet_url  tweet_smiley  tweet_mention  tweet_hashtag  \\\n",
       "target                                                                       \n",
       "0.0     5276.446338   0.503224      0.012437       0.399355       0.384846   \n",
       "1.0     5661.608071   0.760012      0.003057       0.254968       0.509324   \n",
       "\n",
       "        tweet_characters  tweet_words  tweet_av_word_length  \n",
       "target                                                       \n",
       "0.0            78.244818    13.487333              4.767997  \n",
       "1.0            86.497401    13.848059              5.207632  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7d993f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,6))\n",
    "\n",
    "df.groupby('target').mean()['tweet_url'].plot.barh(ax=ax)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf29d72",
   "metadata": {},
   "source": [
    "#Â VISUALISE MORE!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa206db9",
   "metadata": {},
   "source": [
    "### To-do list:\n",
    "\n",
    "- create broader categories for the keyword and, potentially, location columns\n",
    "- use more visualizations through the data cleaning process (to start with: countvectorize before any data cleaning has started to show the words that appear the most frequently)\n",
    "\n",
    "\n",
    "#### Text Pre-processing\n",
    "\n",
    "- ~~check the language that the tweet is written in~~\n",
    "- ~~remove digits~~\n",
    "- ~~expand contractions~~\n",
    "- ~~convert to lowercase~\n",
    "- ~~remove punctuation~~ (maybe include meta-data for punctuation instead?)\n",
    "- ~~tokenize words~~\n",
    "- ~~lemmatize words~~\n",
    "- ~~remove stop-words~~\n",
    "- ~~hashtag extraction~~\n",
    "\n",
    "- ~~does the text contain emojis?~~\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "- ~~meta-data~~\n",
    "    - - ~~how many hash-tags each tweet contains~~\n",
    "    - ~~no. of emojis~~\n",
    "    - ~~number of words~~\n",
    "    - ~~number of characters~~\n",
    "- convert keyword column using techniques discussed here: https://www.kaggle.com/bandits/using-keywords-for-prediction-improvement\n",
    "- ~~average word length~~\n",
    "- use spacy to extract location from location variable\n",
    "\n",
    "#### EDA\n",
    "\n",
    "- word clouds for each target variable\n",
    "- seperate the below by each target variable\n",
    "    - number of characters in each tweet\n",
    "    - average word length in each sentence\n",
    "    - most commonly appearing ngrams of various lenghts\n",
    "    - textblob for sentiment analysis\n",
    "    - use speech tagging\n",
    "    - frequency of most common words\n",
    "    - number of words with a given number of appearances\n",
    "    \n",
    "#### Other\n",
    "\n",
    "- Research the use of LDA and NMF\n",
    "    \n",
    "    \n",
    "Useful articles: \n",
    "\n",
    "https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28\n",
    "\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "https://medium.com/spatial-data-science/how-to-extract-locations-from-text-with-natural-language-processing-9b77035b3ea4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef759a6",
   "metadata": {},
   "source": [
    "TBC: https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
