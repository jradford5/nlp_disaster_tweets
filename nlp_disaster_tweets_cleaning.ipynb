{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dccb61",
   "metadata": {},
   "source": [
    "Data sourced from Kaggle competition [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d54f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f6db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992c4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics/evaluation\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f9a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the train and test sets\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test =  pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b056ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â combining the train and test sets for the purpose of EDA and Data Cleaning/Feature Engineering\n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff5bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataframe Shape: (7613, 5)\n",
      "Test Dataframe Shape: (3263, 4)\n",
      "Combined Dataframe Shape: (10876, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataframe Shape: {}\".format(str(train.shape)))\n",
    "print(\"Test Dataframe Shape: {}\".format(str(test.shape)))\n",
    "print(\"Combined Dataframe Shape: {}\".format(str(df.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0e5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tweets\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf5faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10876 entries, 0 to 10875\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        10876 non-null  int64  \n",
      " 1   keyword   10789 non-null  object \n",
      " 2   location  7238 non-null   object \n",
      " 3   text      10876 non-null  object \n",
      " 4   target    7613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 425.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4aaee5",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7524c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       87\n",
       "location    3638\n",
       "text           0\n",
       "target      3263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values in the training set\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76503b",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790ea358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8022c5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    42\n",
       "0.0    19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to understand whether the null values in keyword have any relevance - they don't\n",
    "\n",
    "df[df.keyword.isnull()].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcc08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new category for the null keyword and location values\n",
    "\n",
    "df.fillna({'keyword': 'unknown', 'location': 'unknown'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080f63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the keyword column\n",
    "\n",
    "df.replace({'keyword': '%20'}, {'keyword': '_'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b56ce0",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae61b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown            3638\n",
       "USA                 141\n",
       "New York            109\n",
       "United States        65\n",
       "London               58\n",
       "Canada               42\n",
       "Nigeria              40\n",
       "Worldwide            35\n",
       "India                35\n",
       "Los Angeles, CA      34\n",
       "UK                   33\n",
       "Kenya                32\n",
       "Washington, DC       31\n",
       "Mumbai               28\n",
       "United Kingdom       26\n",
       "California           25\n",
       "Australia            25\n",
       "Los Angeles          24\n",
       "Chicago, IL          23\n",
       "San Francisco        23\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given how messy and the location column is, it's unlikely that we'll be able to clean it for modelling purposes\n",
    "\n",
    "df.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e0291",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c30c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all tweets are in English\n",
    "\n",
    "# lang_series = df.text.apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbb31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lang_series as a joblib file\n",
    "\n",
    "# joblib.dump(lang_series, 'jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fd48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading lang_series jlib file\n",
    "\n",
    "lang_series = joblib.load('jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4c25239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = lang_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c09e030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>6388</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>#1 Vacation Destination,HAWAII</td>\n",
       "      <td>HURRICANE GUILLERMO LIVE NOAA TRACKING / LOOPI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>7707</td>\n",
       "      <td>panicking</td>\n",
       "      <td>Oxford / bristol</td>\n",
       "      <td>Okay NOW I AM PANICKING</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>7250</td>\n",
       "      <td>nuclear_disaster</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Nuclear deal disaster.\\n\\n#IranDeal #NoNuclear...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>84</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>unknown</td>\n",
       "      <td>SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6593</th>\n",
       "      <td>9441</td>\n",
       "      <td>survivors</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Remembrance  http://t.co/ii4EwE1QIr #Hiroshima...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id           keyword                        location  \\\n",
       "4492  6388         hurricane  #1 Vacation Destination,HAWAII   \n",
       "5401  7707         panicking                Oxford / bristol   \n",
       "5083  7250  nuclear_disaster                         unknown   \n",
       "7637    84            ablaze                         unknown   \n",
       "6593  9441         survivors                         unknown   \n",
       "\n",
       "                                                   text  target language  \n",
       "4492  HURRICANE GUILLERMO LIVE NOAA TRACKING / LOOPI...     1.0       vi  \n",
       "5401                            Okay NOW I AM PANICKING     0.0       tl  \n",
       "5083  Nuclear deal disaster.\\n\\n#IranDeal #NoNuclear...     0.0       id  \n",
       "7637       SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC     NaN       de  \n",
       "6593  Remembrance  http://t.co/ii4EwE1QIr #Hiroshima...     1.0       pt  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.language != 'en'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88469b7",
   "metadata": {},
   "source": [
    "It seems that the language detector function isn't doing a very good job of picking up some of the tweets' language. Regardless, it seems that all of the tweets are in English so we don't have to worry about dealing with other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a1d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping language column from dataset\n",
    "\n",
    "df.drop('language', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f9b66",
   "metadata": {},
   "source": [
    "### Using the tweet-preprocessor package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa57d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14114371",
   "metadata": {},
   "source": [
    "### Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823df9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the tweet characteristics below from the tweets\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b109fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wholesale Markets ablaze'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.clean(df.text[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c781418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df.text.apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93410d0",
   "metadata": {},
   "source": [
    "### Creating meta-data for tweet characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f784e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenized = df.text.apply(lambda x:p.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for-loop to add columns for the tweet meta-data features\n",
    "\n",
    "for feature in ['url', 'hashtag', 'smiley', 'mention']:\n",
    "    feature_counter = []\n",
    "    for tweet in tweet_tokenized:\n",
    "        counter = 0\n",
    "        for word in tweet.split():\n",
    "            if word == \"$\"+feature.upper()+\"$\":\n",
    "                counter += 1\n",
    "        feature_counter.append(counter)\n",
    "    df[\"tweet_\"+feature] = feature_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed2c20",
   "metadata": {},
   "source": [
    "### Text meta-data: length of tweet, number of words and average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b8d806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5965494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_characters'] = df.text_clean.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d60b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(tweet):   \n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()      \n",
    "    return len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93c045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_words'] = df.text_clean.apply(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96421055",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''.join([x for x in 'Our Deeds are the Reason of this #earthquake ' if x not in string.punctuation]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8eeaad9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, words))/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0304654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_word_length(tweet):\n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()\n",
    "    return sum(map(len, word_lst))/len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be3650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_av_word_length'] = df.text_clean.apply(ave_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196de66b",
   "metadata": {},
   "source": [
    "### Remove punctuation completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8586a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation from tweets\n",
    "\n",
    "for punct in string.punctuation:\n",
    "    df['text_clean'] = df.text_clean.str.replace(punct,'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d754958",
   "metadata": {},
   "source": [
    "###Â Removing digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9efdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_num'] = df.text_clean.replace('\\d+','xxxxnumber',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807d274",
   "metadata": {},
   "source": [
    "### Expanding contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb7afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_contr'] = df.no_num.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd3b3b",
   "metadata": {},
   "source": [
    "### Tokenizing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c1665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text_no_contr.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0479d0a",
   "metadata": {},
   "source": [
    "### Change to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "832438f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = df.tokenized.apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17281088",
   "metadata": {},
   "source": [
    "### Beginning the lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d1d42c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(our, PRP$), (deeds, NNS), (are, VBP), (the, ...\n",
       "1        [(forest, JJS), (fire, NN), (near, IN), (la, J...\n",
       "2        [(all, DT), (residents, NNS), (asked, VBD), (t...\n",
       "3        [(xxxxnumber, JJ), (people, NNS), (receive, VB...\n",
       "4        [(just, RB), (got, VBN), (sent, VBD), (this, D...\n",
       "                               ...                        \n",
       "10871    [(earthquake, NN), (safety, NN), (los, NN), (a...\n",
       "10872    [(storm, NN), (in, IN), (ri, NN), (worse, JJR)...\n",
       "10873    [(green, JJ), (line, NN), (derailment, NN), (i...\n",
       "10874    [(meg, NN), (issues, NNS), (hazardous, JJ), (w...\n",
       "10875    [(cityofcalgary, NN), (has, VBZ), (activated, ...\n",
       "Name: lower, Length: 10876, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower.apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ee2c1",
   "metadata": {},
   "source": [
    "TBC: https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01936082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875a6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cd8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c7b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa206db9",
   "metadata": {},
   "source": [
    "### To-do list:\n",
    "\n",
    "- create broader categories for the keyword and, potentially, location columns\n",
    "- use more visualizations through the data cleaning process (to start with: countvectorize before any data cleaning has started to show the words that appear the most frequently)\n",
    "\n",
    "\n",
    "#### Text Pre-processing\n",
    "\n",
    "- ~~check the language that the tweet is written in~~\n",
    "- ~~remove digits~~\n",
    "- ~~expand contractions~~\n",
    "- ~~convert to lowercase~\n",
    "- ~~remove punctuation~~ (maybe include meta-data for punctuation instead?)\n",
    "- ~~tokenize words~~\n",
    "- lemmatize words\n",
    "- remove stop-words\n",
    "- ~~hashtag extraction~~\n",
    "\n",
    "- does the text contain emojis?\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "- meta-data\n",
    "    - - ~~how many hash-tags each tweet contains~~\n",
    "    - ~~no. of emojis~~\n",
    "    - ~~number of words~~\n",
    "    - ~~number of characters~~\n",
    "- sentiment analysis (textblob)\n",
    "- ~~average word length~~\n",
    "- use spacy to extract location from location variable\n",
    "\n",
    "#### EDA\n",
    "\n",
    "- word clouds for each target variable\n",
    "- seperate the below by each target variable\n",
    "    - number of characters in each tweet\n",
    "    - average word length in each sentence\n",
    "    - most commonly appearing ngrams of various lenghts\n",
    "    - textblob for sentiment analysis\n",
    "    - use speech tagging\n",
    "    - frequency of most common words\n",
    "    - number of words with a given number of appearances\n",
    "    \n",
    "#### Other\n",
    "\n",
    "- Research the use of LDA and NMF\n",
    "    \n",
    "    \n",
    "Useful articles: \n",
    "\n",
    "https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28\n",
    "\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "https://medium.com/spatial-data-science/how-to-extract-locations-from-text-with-natural-language-processing-9b77035b3ea4\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
