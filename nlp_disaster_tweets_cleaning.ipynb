{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dccb61",
   "metadata": {},
   "source": [
    "Data sourced from Kaggle competition [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d54f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f6db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389922b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992c4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics/evaluation\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f9a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the train and test sets\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test =  pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b056ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the train and test sets for the purpose of EDA and Data Cleaning/Feature Engineering\n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff5bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataframe Shape: (7613, 5)\n",
      "Test Dataframe Shape: (3263, 4)\n",
      "Combined Dataframe Shape: (10876, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataframe Shape: {}\".format(str(train.shape)))\n",
    "print(\"Test Dataframe Shape: {}\".format(str(test.shape)))\n",
    "print(\"Combined Dataframe Shape: {}\".format(str(df.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0e5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tweets\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf5faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10876 entries, 0 to 10875\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        10876 non-null  int64  \n",
      " 1   keyword   10789 non-null  object \n",
      " 2   location  7238 non-null   object \n",
      " 3   text      10876 non-null  object \n",
      " 4   target    7613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 425.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4aaee5",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7524c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       87\n",
       "location    3638\n",
       "text           0\n",
       "target      3263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values in the training set\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76503b",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790ea358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8022c5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    42\n",
       "0.0    19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to understand whether the null values in keyword have any relevance - they don't\n",
    "\n",
    "df[df.keyword.isnull()].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcc08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new category for the null keyword and location values\n",
    "\n",
    "df.fillna({'keyword': 'unknown', 'location': 'unknown'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080f63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the keyword column\n",
    "\n",
    "df.replace({'keyword': '%20'}, {'keyword': '_'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b56ce0",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae61b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown            3638\n",
       "USA                 141\n",
       "New York            109\n",
       "United States        65\n",
       "London               58\n",
       "Canada               42\n",
       "Nigeria              40\n",
       "Worldwide            35\n",
       "India                35\n",
       "Los Angeles, CA      34\n",
       "UK                   33\n",
       "Kenya                32\n",
       "Washington, DC       31\n",
       "Mumbai               28\n",
       "United Kingdom       26\n",
       "Australia            25\n",
       "California           25\n",
       "Los Angeles          24\n",
       "Chicago, IL          23\n",
       "San Francisco        23\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given how messy and the location column is, it's unlikely that we'll be able to clean it for modelling purposes\n",
    "\n",
    "df.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e0291",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c30c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all tweets are in English\n",
    "\n",
    "# lang_series = df.text.apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbb31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lang_series as a joblib file\n",
    "\n",
    "# joblib.dump(lang_series, 'jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fd48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading lang_series jlib file\n",
    "\n",
    "lang_series = joblib.load('jlib_files/lang_series.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4c25239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = lang_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c09e030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8492</th>\n",
       "      <td>2901</td>\n",
       "      <td>damage</td>\n",
       "      <td>Buzz City</td>\n",
       "      <td>@VZWSupport Zero damage just a horrible product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>8839</td>\n",
       "      <td>sirens</td>\n",
       "      <td>unknown</td>\n",
       "      <td>sleeping with sirens vai vir pra sp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>5452</td>\n",
       "      <td>first_responders</td>\n",
       "      <td>Basking Ridge, NJ</td>\n",
       "      <td>First Responders get int @bridgeportspeed free...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8913</th>\n",
       "      <td>4283</td>\n",
       "      <td>drowning</td>\n",
       "      <td>130515 Û¢ Gallavich.</td>\n",
       "      <td>AND I'M DROWNING IN THE DÌäJÌÛ VUUUUU WE'VE SE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>9605</td>\n",
       "      <td>thunder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Thunder???</td>\n",
       "      <td>0.0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id           keyword               location  \\\n",
       "8492  2901            damage             Buzz City    \n",
       "6192  8839            sirens                unknown   \n",
       "3832  5452  first_responders      Basking Ridge, NJ   \n",
       "8913  4283          drowning  130515 Û¢ Gallavich.   \n",
       "6705  9605           thunder                unknown   \n",
       "\n",
       "                                                   text  target language  \n",
       "8492    @VZWSupport Zero damage just a horrible product     NaN       fr  \n",
       "6192                sleeping with sirens vai vir pra sp     0.0       af  \n",
       "3832  First Responders get int @bridgeportspeed free...     0.0       nl  \n",
       "8913  AND I'M DROWNING IN THE DÌäJÌÛ VUUUUU WE'VE SE...     NaN       de  \n",
       "6705                                         Thunder???     0.0       de  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.language != 'en'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88469b7",
   "metadata": {},
   "source": [
    "It seems that the language detector function isn't doing a very good job of picking up some of the tweets' language. Regardless, it seems that all of the tweets are in English so we don't have to worry about dealing with other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a1d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping language column from dataset\n",
    "\n",
    "df.drop('language', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f9b66",
   "metadata": {},
   "source": [
    "### Using the tweet-preprocessor package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa57d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14114371",
   "metadata": {},
   "source": [
    "### Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823df9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the tweet characteristics below from the tweets\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b109fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wholesale Markets ablaze'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.clean(df.text[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c781418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df.text.apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93410d0",
   "metadata": {},
   "source": [
    "### Creating meta-data for tweet characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f784e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenized = df.text.apply(lambda x:p.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for-loop to add columns for the tweet meta-data features\n",
    "\n",
    "for feature in ['url', 'hashtag', 'smiley', 'mention']:\n",
    "    feature_counter = []\n",
    "    for tweet in tweet_tokenized:\n",
    "        counter = 0\n",
    "        for word in tweet.split():\n",
    "            if word == \"$\"+feature.upper()+\"$\":\n",
    "                counter += 1\n",
    "        feature_counter.append(counter)\n",
    "    df[\"tweet_\"+feature] = feature_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed2c20",
   "metadata": {},
   "source": [
    "### Text meta-data: length of tweet, number of words and average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b8d806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5965494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_characters'] = df.text_clean.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d60b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(tweet):   \n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()      \n",
    "    return len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93c045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_words'] = df.text_clean.apply(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96421055",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''.join([x for x in 'Our Deeds are the Reason of this #earthquake ' if x not in string.punctuation]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8eeaad9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, words))/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0304654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_word_length(tweet):\n",
    "    no_punct = ''.join([x for x in tweet if x not in string.punctuation])\n",
    "    word_lst = no_punct.split()\n",
    "    return sum(map(len, word_lst))/len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be3650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_av_word_length'] = df.text_clean.apply(ave_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196de66b",
   "metadata": {},
   "source": [
    "### Remove punctuation completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8586a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation from tweets\n",
    "\n",
    "for punct in string.punctuation:\n",
    "    df['text_clean'] = df.text_clean.str.replace(punct,'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d754958",
   "metadata": {},
   "source": [
    "### Removing digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9efdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_num'] = df.text_clean.replace('\\d+','',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807d274",
   "metadata": {},
   "source": [
    "### Expanding contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb7afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_contr'] = df.no_num.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd3b3b",
   "metadata": {},
   "source": [
    "### Tokenizing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c1665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text_no_contr.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0479d0a",
   "metadata": {},
   "source": [
    "### Change to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "832438f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = df.tokenized.apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7a8b7",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01936082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad08dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jradford/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62beefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1afdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stop'] = df.lower.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c42ba",
   "metadata": {},
   "source": [
    "### Beginning the lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1a9e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6947e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e804042e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['pos_tag'] = df.no_stop.apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f875a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11044ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('going', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "945cd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "df['wordnet_pos'] = df['pos_tag'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0548c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = df.wordnet_pos.apply(lambda x: ' '.join([wnl.lemmatize(word, tag) for word,tag in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "856c7b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_hashtag</th>\n",
       "      <th>tweet_smiley</th>\n",
       "      <th>tweet_mention</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_words</th>\n",
       "      <th>tweet_av_word_length</th>\n",
       "      <th>no_num</th>\n",
       "      <th>text_no_contr</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_stop</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[(deeds, NNS), (reason, NN), (earthquake, NN),...</td>\n",
       "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[(forest, JJS), (fire, NN), (near, IN), (la, J...</td>\n",
       "      <td>[(forest, a), (fire, n), (near, n), (la, a), (...</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4.954545</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[(residents, NNS), (asked, VBD), (shelter, JJ)...</td>\n",
       "      <td>[(residents, n), (asked, v), (shelter, a), (pl...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>[(people, NNS), (receive, VBP), (wildfires, NN...</td>\n",
       "      <td>[(people, n), (receive, v), (wildfires, n), (e...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[(got, VBD), (sent, JJ), (photo, NN), (ruby, N...</td>\n",
       "      <td>[(got, v), (sent, a), (photo, n), (ruby, n), (...</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   1  unknown  unknown  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  unknown  unknown             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  unknown  unknown  All residents asked to 'shelter in place' are ...   \n",
       "3   6  unknown  unknown  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  unknown  unknown  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  tweet_url  \\\n",
       "0     1.0  Our Deeds are the Reason of this earthquake Ma...          0   \n",
       "1     1.0              Forest fire near La Ronge Sask Canada          0   \n",
       "2     1.0  All residents asked to shelter in place are be...          0   \n",
       "3     1.0  13000 people receive wildfires evacuation orde...          0   \n",
       "4     1.0  Just got sent this photo from Ruby Alaska as s...          0   \n",
       "\n",
       "   tweet_hashtag  tweet_smiley  tweet_mention  ...  tweet_words  \\\n",
       "0              0             0              0  ...           13   \n",
       "1              0             0              0  ...            7   \n",
       "2              0             0              0  ...           22   \n",
       "3              0             0              0  ...            8   \n",
       "4              0             0              0  ...           16   \n",
       "\n",
       "   tweet_av_word_length                                             no_num  \\\n",
       "0              4.307692  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1              4.428571              Forest fire near La Ronge Sask Canada   \n",
       "2              4.954545  All residents asked to shelter in place are be...   \n",
       "3              6.875000   people receive wildfires evacuation orders in...   \n",
       "4              4.375000  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                       text_no_contr  \\\n",
       "0  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  All residents asked to shelter in place are be...   \n",
       "3  people receive wildfires evacuation orders in ...   \n",
       "4  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Our, Deeds, are, the, Reason, of, this, earth...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, residents, asked, to, shelter, in, place...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                             no_stop  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [people, receive, wildfires, evacuation, order...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(deeds, NNS), (reason, NN), (earthquake, NN),...   \n",
       "1  [(forest, JJS), (fire, NN), (near, IN), (la, J...   \n",
       "2  [(residents, NNS), (asked, VBD), (shelter, JJ)...   \n",
       "3  [(people, NNS), (receive, VBP), (wildfires, NN...   \n",
       "4  [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(deeds, n), (reason, n), (earthquake, n), (ma...   \n",
       "1  [(forest, a), (fire, n), (near, n), (la, a), (...   \n",
       "2  [(residents, n), (asked, v), (shelter, a), (pl...   \n",
       "3  [(people, n), (receive, v), (wildfires, n), (e...   \n",
       "4  [(got, v), (sent, a), (photo, n), (ruby, n), (...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  people receive wildfire evacuation order calif...  \n",
       "4  get sent photo ruby alaska smoke wildfires pou...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa206db9",
   "metadata": {},
   "source": [
    "### To-do list:\n",
    "\n",
    "- create broader categories for the keyword and, potentially, location columns\n",
    "- use more visualizations through the data cleaning process (to start with: countvectorize before any data cleaning has started to show the words that appear the most frequently)\n",
    "\n",
    "\n",
    "#### Text Pre-processing\n",
    "\n",
    "- ~~check the language that the tweet is written in~~\n",
    "- ~~remove digits~~\n",
    "- ~~expand contractions~~\n",
    "- ~~convert to lowercase~\n",
    "- ~~remove punctuation~~ (maybe include meta-data for punctuation instead?)\n",
    "- ~~tokenize words~~\n",
    "- ~~lemmatize words~~\n",
    "- ~~remove stop-words~~\n",
    "- ~~hashtag extraction~~\n",
    "\n",
    "- ~~does the text contain emojis?~~\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "- meta-data\n",
    "    - - ~~how many hash-tags each tweet contains~~\n",
    "    - ~~no. of emojis~~\n",
    "    - ~~number of words~~\n",
    "    - ~~number of characters~~\n",
    "- sentiment analysis (textblob)\n",
    "- ~~average word length~~\n",
    "- use spacy to extract location from location variable\n",
    "\n",
    "#### EDA\n",
    "\n",
    "- word clouds for each target variable\n",
    "- seperate the below by each target variable\n",
    "    - number of characters in each tweet\n",
    "    - average word length in each sentence\n",
    "    - most commonly appearing ngrams of various lenghts\n",
    "    - textblob for sentiment analysis\n",
    "    - use speech tagging\n",
    "    - frequency of most common words\n",
    "    - number of words with a given number of appearances\n",
    "    \n",
    "#### Other\n",
    "\n",
    "- Research the use of LDA and NMF\n",
    "    \n",
    "    \n",
    "Useful articles: \n",
    "\n",
    "https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28\n",
    "\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "https://medium.com/spatial-data-science/how-to-extract-locations-from-text-with-natural-language-processing-9b77035b3ea4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef759a6",
   "metadata": {},
   "source": [
    "TBC: https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
